{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ff8612c92dd38ad6ac36f41445c32eafe6db4351c06d677ed5f1ad3d38827b4b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import re\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import string\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 article  \\\n",
       "0      Its official: US President Barack Obama wants ...   \n",
       "1      (CNN) -- Usain Bolt rounded off the world cham...   \n",
       "2      Kansas City, Missouri (CNN) -- The General Ser...   \n",
       "3      Los Angeles (CNN) -- A medical doctor in Vanco...   \n",
       "4      (CNN) -- Police arrested another teen Thursday...   \n",
       "...                                                  ...   \n",
       "19995  (CNN) -- A new Lebanese government was announc...   \n",
       "19996  (CNN) -- Jaycee Dugard filed a complaint again...   \n",
       "19997  (CNN)Sky watchers in western North America are...   \n",
       "19998  Tripoli, Libya (CNN) -- A political coalition ...   \n",
       "19999  (CNN) -- Roger Federer finally clinched his fi...   \n",
       "\n",
       "                                          summary_string  \\\n",
       "0      Syrian official: Obama climbed to the top of t...   \n",
       "1      Usain Bolt wins third gold of world championsh...   \n",
       "2      The employee in agencys Kansas City office is ...   \n",
       "3      NEW: A Canadian doctor says she was part of a ...   \n",
       "4      Another arrest made in gang rape outside Calif...   \n",
       "...                                                  ...   \n",
       "19995  A new government is announced, bringing to an ...   \n",
       "19996  NEW: Abuse is due to \"the U.S. parole commissi...   \n",
       "19997  The total eclipse will only last 4 minutes and...   \n",
       "19998  Libyans voted July 7 for a General National Co...   \n",
       "19999  Roger Federer defeats Jo-Wilfried Tsonga to cl...   \n",
       "\n",
       "                                            summary_list  \n",
       "0      ['Syrian official: Obama climbed to the top of...  \n",
       "1      ['Usain Bolt wins third gold of world champion...  \n",
       "2      ['The employee in agencys Kansas City office i...  \n",
       "3      ['NEW: A Canadian doctor says she was part of ...  \n",
       "4      ['Another arrest made in gang rape outside Cal...  \n",
       "...                                                  ...  \n",
       "19995  ['A new government is announced, bringing to a...  \n",
       "19996  ['NEW: Abuse is due to \"the U.S. parole commis...  \n",
       "19997  ['The total eclipse will only last 4 minutes a...  \n",
       "19998  ['Libyans voted July 7 for a General National ...  \n",
       "19999  ['Roger Federer defeats Jo-Wilfried Tsonga to ...  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>summary_string</th>\n      <th>summary_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Its official: US President Barack Obama wants ...</td>\n      <td>Syrian official: Obama climbed to the top of t...</td>\n      <td>['Syrian official: Obama climbed to the top of...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(CNN) -- Usain Bolt rounded off the world cham...</td>\n      <td>Usain Bolt wins third gold of world championsh...</td>\n      <td>['Usain Bolt wins third gold of world champion...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kansas City, Missouri (CNN) -- The General Ser...</td>\n      <td>The employee in agencys Kansas City office is ...</td>\n      <td>['The employee in agencys Kansas City office i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Los Angeles (CNN) -- A medical doctor in Vanco...</td>\n      <td>NEW: A Canadian doctor says she was part of a ...</td>\n      <td>['NEW: A Canadian doctor says she was part of ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(CNN) -- Police arrested another teen Thursday...</td>\n      <td>Another arrest made in gang rape outside Calif...</td>\n      <td>['Another arrest made in gang rape outside Cal...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>(CNN) -- A new Lebanese government was announc...</td>\n      <td>A new government is announced, bringing to an ...</td>\n      <td>['A new government is announced, bringing to a...</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>(CNN) -- Jaycee Dugard filed a complaint again...</td>\n      <td>NEW: Abuse is due to \"the U.S. parole commissi...</td>\n      <td>['NEW: Abuse is due to \"the U.S. parole commis...</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>(CNN)Sky watchers in western North America are...</td>\n      <td>The total eclipse will only last 4 minutes and...</td>\n      <td>['The total eclipse will only last 4 minutes a...</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>Tripoli, Libya (CNN) -- A political coalition ...</td>\n      <td>Libyans voted July 7 for a General National Co...</td>\n      <td>['Libyans voted July 7 for a General National ...</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>(CNN) -- Roger Federer finally clinched his fi...</td>\n      <td>Roger Federer defeats Jo-Wilfried Tsonga to cl...</td>\n      <td>['Roger Federer defeats Jo-Wilfried Tsonga to ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('article_summaries_noNaN_prep_20000_2.csv', encoding = 'utf-8', escapechar='\\\\', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Translates multiple whitespace into single space character..\n",
    "    \"\"\"\n",
    "    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n",
    "\n",
    "\n",
    "def _replace_whitespace(match):\n",
    "    text = match.group()\n",
    "\n",
    "    if \"\\n\" in text or \"\\r\" in text:\n",
    "        return \"\\n\"\n",
    "\n",
    "    else:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(comment):\n",
    "\n",
    "    comment = re.sub(r'\\n', ' ', comment)\n",
    "    comment = re.sub(r'\\r', ' ', comment)\n",
    "    comment = re.sub(r'\\$', ' ', comment)\n",
    "    comment = re.sub(r'\\€', ' ', comment)\n",
    "    comment = comment.replace('(', ' ')\n",
    "    comment = comment.replace(')', ' ')\n",
    "    comment = comment.replace('[', ' ')\n",
    "    comment = comment.replace(']', ' ')\n",
    "    comment = comment.replace('<', ' ')\n",
    "    comment = comment.replace('>', ' ')\n",
    "    comment = comment.replace('-', ' ')\n",
    "    comment = comment.encode('ascii', 'ignore').decode('ascii') #remove non-ascii char\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess(text):\n",
    "\n",
    "    text = normalize_whitespace(text)\n",
    "    text = remove_symbols(text)\n",
    "    comment_tokens = word_tokenize(text)\n",
    "    comment_tokens = [token.strip().replace('``', ' ' ) for token in comment_tokens]\n",
    "    comment= ' '.join(map(str, comment_tokens))\n",
    "    comment_nowhite = normalize_whitespace(comment)\n",
    "\n",
    "    return comment_nowhite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop = set(stop)\n",
    "stop.add('\"')\n",
    "stop.add(\"'\")\n",
    "stop = list(stop)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_string(text):\n",
    "    text = text.replace('US', 'USA')\n",
    "    text = normalize_whitespace(text)\n",
    "    text = remove_symbols(text)\n",
    "    text = re.sub(r'\\d', '', text) #remove numbers\n",
    "    comment_tokens = word_tokenize(text) #tokenize\n",
    "    comment_tokens = [token.strip().replace('``', ' ' ) for token in comment_tokens]\n",
    "    comment_tokens = [token.lower() for token in comment_tokens] #transform in lowercase\n",
    "    comment_tokens_no_stopwords = [item for item in comment_tokens if item not in stop] #remove stopwords\n",
    "    comment_lemma = [lemmatizer.lemmatize(item) for item in comment_tokens_no_stopwords] #comment_tokens_no_stopwords\n",
    "    comment_lemma = [item for item in comment_lemma if item not in string.punctuation] #remove punctuation\n",
    "    comment_lemma = [normalize_whitespace(item).strip() for item in comment_lemma]\n",
    "    comment_lemma = ' '.join(map(str, comment_lemma))\n",
    "\n",
    "    return comment_lemma"
   ]
  },
  {
   "source": [
    "texts_processed = df['article'].apply(lambda x: preprocess_string(x))\n",
    "with open('texts_processed20000.pickle', 'wb') as handle:\n",
    "    pickle.dump(texts_processed, handle)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "df['text_processed'] = texts_processed"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df= 0.5 ,smooth_idf = True)\n",
    "tfidf_vectorizer_fit = tfidf_vectorizer.fit(df['text_processed']) #fit tfidf vectorizer on processed texts\n",
    "with open('tfidf_vect_fit_20000.pickle', 'wb') as handle:\n",
    "    pickle.dump(tfidf_vectorizer_fit, handle)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#with open('tfidf_vect_fit_20000.pickle', 'rb') as handle:\n",
    "#    tfidf_vectorizer_fit = pickle.load(handle)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix_cosine_tfidf(sentences):\n",
    "\n",
    "    # Generate the tf-idf vectors for the corpus\n",
    "    tfidf_matrix = tfidf_vectorizer_fit.transform(sentences) \n",
    "\n",
    "    # compute the cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    for i in range(len(cosine_sim_matrix)):\n",
    "        for j in range(len(cosine_sim_matrix)):\n",
    "            if i == j:\n",
    "                cosine_sim_matrix[i,j] = 0 # let similarity between the same sentence be 0\n",
    "    \n",
    "    return cosine_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences(text, number=10):\n",
    "    \n",
    "    text_ = simple_preprocess(text) \n",
    "    sentences = sent_tokenize(text_)\n",
    "\n",
    "    sentences_preprocess = [preprocess_string(item) for item in sentences] #complete preprocess on every sentence\n",
    "\n",
    "    sm = build_similarity_matrix_cosine_tfidf(sentences_preprocess)\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(sm) #build the graph\n",
    "    pr_vector = nx.pagerank(nx_graph) #apply PageRank\n",
    "    \n",
    "    top_sentences_idx = {}\n",
    "    for i in range(len(sentences)):\n",
    "        top_sentences_idx[sentences[i]]=i #store original senteces and their order in a dictionary\n",
    "\n",
    "    top_sentences = []\n",
    "    if pr_vector is not None:\n",
    "\n",
    "        sorted_pr = sorted(((pr_vector[i],s) for i,s in enumerate(sentences)), reverse=True) #sort in descending order\n",
    "\n",
    "        if len(sentences) <= number: #if number of sentences in text is less than 10 (or number set by the user)\n",
    "            number = len(sentences)\n",
    "\n",
    "        for epoch in range(number):\n",
    "            sent = sorted_pr[epoch][1]\n",
    "            top_sentences.append(sent)\n",
    "\n",
    "    return top_sentences, top_sentences_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_get_top_sentences(text, top_sentence_prev, count_dict, number = 10):\n",
    "\n",
    "    \"\"\"\n",
    "    Check if top sentences are almost equal between them  \n",
    "        (es. ['Phone: xxxx', 'Phone: xxxx', 'Phone: xxxx']) \n",
    "    and in that case remove that sentences from text.\n",
    "    \"\"\"\n",
    "    text = simple_preprocess(text)\n",
    "    freq = list(count_dict.values())\n",
    "    for el in freq:\n",
    "        for key in count_dict:\n",
    "            if count_dict[key] == el:\n",
    "                if el > sum(freq)/2:\n",
    "                    text_new = text.replace (key, ' ')\n",
    "    top_sentences, top_sentences_idx = get_top_sentences(text_new,  number = number)\n",
    "\n",
    "    return top_sentences, top_sentences_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity_cosine_tfidf (sentence1, sentence2,):\n",
    "\n",
    "    s1 = tfidf_vectorizer_fit.transform([sentence1])\n",
    "    s2 = tfidf_vectorizer_fit.transform([sentence2])\n",
    "\n",
    "    # compute and print the cosine similarity matrix\n",
    "    cosine_sim = cosine_similarity(s1, s2)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_query_to_syn (query):\n",
    "    query = query.lower()\n",
    "    query = lemmatizer.lemmatize(query)\n",
    "\n",
    "    if len(query) > 1: #if query is composed by more than 1 word\n",
    "        s1_list = query.split(\" \")\n",
    "    else:\n",
    "        s1_list = [query]\n",
    "\n",
    "    for w in range(len(s1_list)):  \n",
    "        for syn in wordnet.synsets(s1_list[w]): # add synonims\n",
    "            for l in syn.lemmas():\n",
    "                if l.name() not in s1_list:\n",
    "                    s1_list.append(l.name())\n",
    "\n",
    "    s1_to_string = ' '.join(s1_list)\n",
    "\n",
    "    return s1_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMRScore(candidate, summary, lambta=0.5, query = None):\n",
    "\t\n",
    "\tcandidate = preprocess_string(candidate)\n",
    "\n",
    "\tif query != None : \n",
    "\t\tref = from_query_to_syn(query) #if there is a query, the reference sentence will be that one\n",
    "\telse:\n",
    "\t\tref = preprocess_string(summary[0]) #if there is NOT a query, the reference sentence will be the one with highest importance\n",
    "\n",
    "\tSim1 = sentence_similarity_cosine_tfidf(candidate, ref) #compute similarity between candidate sentence and the reference one\n",
    "\tl_expr = lambta * Sim1\n",
    "\tvalue = [float(\"-inf\")]\n",
    "\n",
    "\tfor sent in summary:\n",
    "\t\tsent = preprocess_string(sent)\n",
    "\t\tSim2 = sentence_similarity_cosine_tfidf(candidate, sent) #compute similarity between candidate sentence and the one that are already in summary\n",
    "\t\tvalue.append(Sim2)\n",
    "\n",
    "\tr_expr = (1-lambta) * max(value)\n",
    "\tMMR_SCORE = l_expr - r_expr\t\n",
    "\n",
    "\treturn MMR_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize (text, max_words = 100, query = None):\n",
    "\n",
    "    top_sentences, top_sentences_idx = get_top_sentences(text)\n",
    "\n",
    "    #build dictionary for 'check_get_top_sentences'\n",
    "    top_sentences_no_num = [re.sub(r'\\d', '', sentence) for sentence in top_sentences]\n",
    "    count_dict = pd.DataFrame(top_sentences_no_num, columns=[\"x\"]).groupby('x').size().to_dict()\n",
    "    flag = any(l > sum(list(count_dict.values()))/2 for l in list(count_dict.values()))\n",
    "    if flag == True:\n",
    "        top_sentences, top_sentences_idx = check_get_top_sentences(text, top_sentences, count_dict)\n",
    "  \n",
    "\n",
    "    summary =[]\n",
    "    list_idx = []\n",
    "\n",
    "    # if there is a query, first sentence will be the one with highest similarity with the query\n",
    "    if query != None:\n",
    "        max_sim = -999\n",
    "        i=0\n",
    "        for candidate_first in top_sentences:\n",
    "            candidate_prep = preprocess_string(candidate_first)\n",
    "            Sim1 = sentence_similarity_cosine_tfidf(candidate_prep, from_query_to_syn(query))\n",
    "            if Sim1 > max_sim:\n",
    "                max_sim = Sim1\n",
    "                first_sentence = candidate_first\n",
    "            i+=1\n",
    "    # if there is NOT a query, first sentence will be the one with highest score importance\n",
    "    else:\n",
    "        first_sentence = top_sentences[0]\n",
    "    \n",
    "    summary.append(first_sentence)\n",
    "    pair = (first_sentence, top_sentences_idx[first_sentence])\n",
    "    list_idx.append(pair)\n",
    "\n",
    "\n",
    "    sum_words = len(re.findall(r'\\w+', first_sentence)) #count number of words in the summary\n",
    "    \n",
    "\n",
    "    while sum_words < max_words:\n",
    "        max_mmr = -9999\n",
    "        for candidate in top_sentences: #for every sentence, compute MMRScore and insert in the summary the one with highest mmrscore\n",
    "            if candidate not in summary:\n",
    "                candidate_prep = preprocess_string(candidate)\n",
    "                mmr = MMRScore(candidate_prep, summary, query=query)\n",
    "                if mmr > max_mmr:\n",
    "                    candidate_sentence = candidate\n",
    "                    max_mmr = mmr\n",
    "\n",
    "        if candidate_sentence in summary:\n",
    "            break\n",
    "        \n",
    "        pair = (candidate_sentence, top_sentences_idx[candidate_sentence])\n",
    "        list_idx.append(pair)\n",
    "\n",
    "        summary.append(candidate_sentence)\n",
    "\n",
    "        num_words = len(re.findall(r'\\w+', candidate_sentence))\n",
    "        sum_words += num_words\n",
    "\n",
    "\n",
    "    sorted_list_idx = sorted(list_idx, key=itemgetter(1))\n",
    "    summary_sorted = [pair[0] for pair in sorted_list_idx] #sort summary\n",
    "\n",
    "    return summary, summary_sorted"
   ]
  },
  {
   "source": [
    "df_prova_summ = pd.DataFrame()\n",
    "summary_list = []\n",
    "text_list = []\n",
    "summ_gt_list = []\n",
    "summ_gt_list_list = []\n",
    "\n",
    "problem_list = []\n",
    "\n",
    "for i in range(0,12000):\n",
    "    \n",
    "    print(i)\n",
    "    try:\n",
    "        text = df.iloc[i,0] #article\n",
    "        summ_gt = df.iloc[i,1] #summary_proposed\n",
    "        summ_gt_lista = df.iloc[i,2]\n",
    "        _,sorted_summary = summarize(text, 75)\n",
    "\n",
    "        sorted_summary = ' '.join(map(str, sorted_summary))\n",
    "\n",
    "        text_list.append(text)\n",
    "        summ_gt_list.append(summ_gt)\n",
    "        summary_list.append(sorted_summary)\n",
    "        summ_gt_list_list.append(summ_gt_lista)\n",
    "    except:\n",
    "        problem_list.append(i)\n",
    "        text_list.append(text)\n",
    "        summ_gt_list.append(summ_gt)\n",
    "        summary_list.append('---')\n",
    "        summ_gt_list_list.append(summ_gt_lista)\n",
    "\n",
    "    if i % 1500 == 0: #save every 1500 iterations\n",
    "        with open('summary_list_12000_final2_nx_{}.pickle'.format(i), 'wb') as handle:\n",
    "            pickle.dump(summary_list, handle)\n",
    "    \n",
    "\n",
    "df_prova_summ['article'] = text_list\n",
    "df_prova_summ['summary_gt'] = summ_gt_list\n",
    "df_prova_summ['summary_gt_list'] = summ_gt_list_list\n",
    "df_prova_summ['summary'] = summary_list\n",
    "\n",
    "with open('df_summary_12000__final2_nx.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_prova_summ, handle)\n",
    "with open('problem_list__final2_nx.pickle', 'wb') as handle:\n",
    "    pickle.dump(problem_list, handle)\n",
    "df_prova_summ.to_csv('df_summary_12000__final2_nx.csv')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## See how it works!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Los Angeles (CNN) -- A medical doctor in Vancouver, British Columbia, said Thursday that California arson suspect Harry Burkhart suffered from severe mental illness in 2010, when she examined him as part of a team of doctors.   Dr.  Blaga Stancheva, a family physician and specialist in obstetrics, said both Burkhart and his mother, Dorothee, were her patients in Vancouver while both were applying for refugee status in Canada.   \"I was asked to diagnose and treat Harry to support a claim explaining why he was unable to show up in a small-claims court case,\" Stancheva told CNN in a phone interview.  She declined to cite the case or Burkharts role in it.   Stancheva said she and other doctors including a psychiatrist diagnosed Burkhart with \"autism, severe anxiety, post-traumatic stress disorder and depression. \" The diagnosis was spelled out in a letter she wrote for the small-claims court case, Stancheva said.   Stancheva, citing doctor-patient confidentiality, would not elaborate further, nor would she identify the psychiatrist involved in the diagnosis.   Burkhart, a 24-year-old German national, has been charged with 37 counts of arson following a string of 52 fires in Los Angeles.  The charges are in connection with arson fires at 12 locations scattered through Hollywood, West Hollywood and Sherman Oaks, according to authorities.   Stancheva said the refugee applications by Burkhart and his mother were denied by the Canadian government, and she has not seen Burkhart since early March of 2010.   \"I was shocked and dismayed at what happened in Los Angeles, and it appears he was not being treated for his depression,\" she said.   Burkhart was in court on Wednesday for a preliminary hearing.   Prosecutors said his \"rage against Americans,\" triggered by his mothers arrest last week, motivated his \"campaign of terror\" with dozens of fires in Hollywood and nearby communities.   Burkhart kept his eyes closed and remained limp during most of his hearing, requiring sheriffs deputies to hold him up.   The district attorney called his courtroom behavior \"very bizarre. \"  \"This defendant has engaged in a protracted campaign in which he has set, the people believe, upwards of 52 arson fires in what essentially amounts to a campaign of terror against this community,\" Los Angeles County Deputy District Attorney Sean Carney said.  \"The people believe he has engaged in this conduct because he has a hatred for Americans. \"  Carney told the court Burkhart would flee the country if he was allowed out of jail on bond, but Los Angeles Superior Court Judge Upinder Kalra said he had no choice but to set bail.  To go free while awaiting trial, Burkhart must post a $xxxx million bond and surrender his German passport.   It was revealed that Burkhart is also under investigation for arson and fraud in relation to a fire in Neukirchen, near Frankfurt, Germany.   The worst arson sprees in the citys history began last Friday morning with a car fire in Hollywood that spread to apartments above a garage, but no new fires have happened since Burkhart was arrested Monday, Los Angeles District Attorney Steve Cooley said.   No one was hurt in the fires, but property damage costs are likely to reach $3 million, authorities said.   Cooley called it \"almost attempted murder,\" because people were sleeping in apartments above where Burkhart allegedly set cars on fire with incendiary devices placed under their engines.   The criminal complaint filed Wednesday also alleged that the fires were \"caused by use of a device designed to accelerate the fire,\" Cooley said.  \"If found true, the allegation could mean additional custody time for the defendant. \"  \"In numerous instances, the cars were parked in carports, resulting in the fires spreading to the adjacent occupied apartment buildings,\" a sworn affidavit from a Los Angeles arson investigator said.  \"The vast majority of these fires occurred late at night when the occupants of the apartment buildings were asleep. \"  Investigator Edward Nordskogs affidavit detailed Burkharts behavior a day before the fires began, when he was in a federal courtroom during extradition proceedings for his mother.   \"While in the audience, the defendant (Burkhart) began yelling in an angry manner, F--k all Americans.  The defendant also attempted to communicate with his mother who was in custody.  Shortly thereafter, the defendant was ejected from the courtroom by Deputy US Marshals,\" Nordskog wrote.   Dorothee Burkhart was arrested a day before on an international arrest warrant issued by a district court in Frankfurt, Germany, said federal court spokesman Gunther Meilinger.  The 53-year-old German woman is wanted on 16 counts of fraud and three counts of embezzlement, he said.   The charges include an allegation that she failed to pay for a breast enhancement operation performed on her in 2004, Meilinger said.  Most of the German charges, however, stem from phony real estate deals that Dorothee Burkhart allegedly conducted between 2000 and 2006.   \"It is my opinion that the defendants criminal spree was motivated by his rage against Americans and that by setting these fires the defendant intended to harm and terrorize as many residents of the city and county of Los Angeles as possible,\" Nordskog wrote.   A search of Burkharts Hollywood apartment found newspaper clippings about the Los Angeles fires and articles from Germany reporting similar car fires in Frankfurt, Germany in September, 2011, the investigator said.   \"It is my opinion based on my experience that it is highly likely the defendant has a history of setting arson fires in Germany before he came to the United States,\" Nordskog wrote.   Burkharts mother is scheduled for another extradition hearing Friday, while he is due back in court for arraignment on January 24.  Meanwhile, both Burkharts are housed in a Los Angeles jail.   '"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df.iloc[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Burkhart , a 24 year old German national , has been charged with 37 counts of arson following a string of 52 fires in Los Angeles .',\n",
       " 'Stancheva said the refugee applications by Burkhart and his mother were denied by the Canadian government , and she has not seen Burkhart since early March of 2010 .',\n",
       " 'It was revealed that Burkhart is also under investigation for arson and fraud in relation to a fire in Neukirchen , near Frankfurt , Germany .',\n",
       " 'The worst arson sprees in the citys history began last Friday morning with a car fire in Hollywood that spread to apartments above a garage , but no new fires have happened since Burkhart was arrested Monday , Los Angeles District Attorney Steve Cooley said .']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "_, sorted_summary = summarize(df.iloc[3,0])\n",
    "sorted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Los Angeles CNN A medical doctor in Vancouver , British Columbia , said Thursday that California arson suspect Harry Burkhart suffered from severe mental illness in 2010 , when she examined him as part of a team of doctors .',\n",
       " 'Burkhart , a 24 year old German national , has been charged with 37 counts of arson following a string of 52 fires in Los Angeles .',\n",
       " 'It was revealed that Burkhart is also under investigation for arson and fraud in relation to a fire in Neukirchen , near Frankfurt , Germany .',\n",
       " 'The worst arson sprees in the citys history began last Friday morning with a car fire in Hollywood that spread to apartments above a garage , but no new fires have happened since Burkhart was arrested Monday , Los Angeles District Attorney Steve Cooley said .']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "_, sorted_summary_arson = summarize(df.iloc[3,0], query = 'arson')\n",
    "sorted_summary_arson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Los Angeles CNN A medical doctor in Vancouver , British Columbia , said Thursday that California arson suspect Harry Burkhart suffered from severe mental illness in 2010 , when she examined him as part of a team of doctors .',\n",
       " 'Stancheva said she and other doctors including a psychiatrist diagnosed Burkhart with autism , severe anxiety , post traumatic stress disorder and depression.',\n",
       " \"Cooley called it almost attempted murder , '' because people were sleeping in apartments above where Burkhart allegedly set cars on fire with incendiary devices placed under their engines .\",\n",
       " 'Dorothee Burkhart was arrested a day before on an international arrest warrant issued by a district court in Frankfurt , Germany , said federal court spokesman Gunther Meilinger .']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "_, sorted_summary_mentaldisorder = summarize(df.iloc[3,0], query = 'mental disorder')\n",
    "sorted_summary_mentaldisorder"
   ]
  }
 ]
}